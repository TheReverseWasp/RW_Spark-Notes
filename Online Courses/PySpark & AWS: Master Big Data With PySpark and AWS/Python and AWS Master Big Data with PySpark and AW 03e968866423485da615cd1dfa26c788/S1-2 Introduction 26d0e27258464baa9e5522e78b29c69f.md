# S1-2: Introduction

# Why Spark?

- Speed
- Distributed
- Advanced Analytics
- Real Time
- Powerful Caching
- Fault Tolerant
- Deployment

# Hadoop

![S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled.png](S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled.png)

- HDFS: Hadoop Distributed File Storage.
- YARN: The OS.
- Map Reduce: Technique to map and reduce data.
    - It has limitations.

![S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%201.png](S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%201.png)

- SPARK: Solve these limitations.

# Spark Architecture

![S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%202.png](S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%202.png)

- Spark Context: Manage nodes and data.
- Cluster Manager: Manage workers.
- Worker

# Spark Ecosystem

![S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%203.png](S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%203.png)

- Spark Core: Se puede escribir en Java, Scala, Python y R con el mismo nivel de abstracción
- Incluye:
    - SparkSQL
    - Spark Streaming → Permite manipular los datos de entrada en tiempo real
    - Spark MLlib: para ML
    - Spark Graphx: Grafos

    # DataBricks

    **Online** o local, de preferencia online

    - Nota: Crear una cuenta en DataBricks Community edition

    ## Clusters

    ![S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%204.png](S1-2%20Introduction%2026d0e27258464baa9e5522e78b29c69f/Untitled%204.png)

    - Darle un nombre
    - Especificar una versión
    - Crear un Notebook
        - Especificar el cluster

    # Spark Local Setup

    - Descargar Java, Python, Spark, Winutils (hadooop → misma version en la que se descargo Spark en el git de hadooop  en el subdirectorio /bin)
    - Para Java en Windows:
        - Setear las variables de entorno al jre, si no hay una variable de JAVA_HOME a la direccion del jre
        - Tambien en variables del sistema dentro de la version de jre buscar el path del bin y guardar
    - Para Python en Windows
        - Setear las variables e instalar
    - Spark en Windows
        - Extraer Spark.
        - Pegar en el disco C el directorio.
        - copiar el path dentro de Spark →
            - Agregar SPARK_HOME a las variables de entorno.
            - el directorio bin agregarlo a las variables del sistema.
    - Para Winutils
        - Crear una carpeta Hadoop en C.
        - Crear una carpeta en Hadoop llamada bin.
        - Pegar Winutils.exe
        - Agregar el directorio Hadoop a las variables de entorno y el bin de Hadoop a las variables del sistema dentro de path

    Nota: Todas las variables del sitema estan dentro de PATH

## Correr Spark

- Correr en la linea de comandos :

```
spark-shell
```

- Para probar PySpark

```
pyspark
```