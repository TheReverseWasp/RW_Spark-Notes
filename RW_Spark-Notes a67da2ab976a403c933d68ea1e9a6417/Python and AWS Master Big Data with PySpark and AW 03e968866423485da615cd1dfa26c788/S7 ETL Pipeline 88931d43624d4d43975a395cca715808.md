# S7: ETL Pipeline

![Untitled](S7%20ETL%20Pipeline%2088931d43624d4d43975a395cca715808/Untitled.png)

![Untitled](S7%20ETL%20Pipeline%2088931d43624d4d43975a395cca715808/Untitled%201.png)

# ETL Pipeline

```python
dbutils.fs.rm("PATH/to/folder", True) 
```

# Extract

```python
# Extract
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("ETL Pipeline").getOrCreate()
spark.read.text("PATH/to/folder/file.txt") #random sentences

df.show()
display(df)
```

# Transform

```python
# Transformation
from pyspark.sql.functions import lit, col, explode
import pyspark.sql.functions as f
```

```python
df2 = df.withColumn("splitedData", f.split("value", " "))
df3 = df2.withColumn("words", explode("splitedData"))
display(df3)
```

```python
wordsDF = df3.select("words")
display(wordsDF)
```

```python
wordsDF.groupBy("words").count().show()
```

# Load

- AWS RDS
- Create a DB instance
- The PostgreSQL option
- Free tier option
- Edit the DB Settings
    - User: postgres
    - Password: Any
- No autoscaling
- Connectivity all by default
    - Public Access â†’ Yes
- Database Authentication
    - Password authentication
- Additional configuration
    - No Backup
    - Monitoring disabled
    - No log Export
    - Unable maintenance
    - Deletion Protection disabled

# Connections

Things you might do...

- Download and install postgres on local.
- Connect AWS RDS to your local postgres pgadmin.
- Create an Schema (pgadmin)
- Specify your endpoint, tablename and driver in your databricks notebook

```python
driver = "org.postgres.Driver"
url = "jdbc:postgresql://endpoint to AWS RDS/" # wirh an slash at the end and the jdbc beginning
table = "Schema_name.Table_name"
user = "postgres"
password = "your aws postgres password"

wordCount.write.format("jdbc").option("driver", driver).option("url", url).option("dbtable", table).option("mode", "append").option("user", user).option("password", password).save()
```

- Now you can see the data from local