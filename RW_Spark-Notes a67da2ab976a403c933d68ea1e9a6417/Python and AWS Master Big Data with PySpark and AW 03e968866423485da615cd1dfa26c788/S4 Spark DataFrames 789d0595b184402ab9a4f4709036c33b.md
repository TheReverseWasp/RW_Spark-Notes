# S4. Spark DataFrames

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled.png)

# Create a Dataframe

```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark Dataframes").getOrCreate()

#1
df = spark.read.csv("PATH/to.csv") #read. have various ooptions

df.show() # similarly to collect but on dfs

#2
df = spark.read.option("header",True).csv("PATH/to.csv") #better / situational
df.show()
```

# Schema of Dataframe

```python
#1 So many options
df = spark.read.option("infoSchema",True).option("header",True).csv("PATH/to.csv") #better / situational
df.printSchema() #in the way to see datatypes of each column
#2 Better
df = spark.read.options(infoSchema='True', header='True').csv('PATH/to.csv')
```

## Other options

- delimiter: In the case of csv could be ',', '\t', etc.

# Providing Schema of Dataframe

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType #and any other types

schema = StructType([
											StructField("age", IntegerType() , True),
											StructField("gender", StringType() , True),
											StructField("name", StringType() , True),
											StructField("course", StringType() , True),
											StructField("roll", StringType() , True),
											StructField("marks", IntegerType() , True),
											StructField("email", StringType() , True),
											
])

#Next
df = spark.read.options(infoSchema='True', header='True').schema(schema).csv('PATH/to.csv')
df.show()
df.printSchema()
```

# Creating DataFrame from RDD

```python
#1
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Spark Dataframes").getOrCreate()
```

```python
#2
from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName("RDD")
sc = SparkContext.getOrCreate(conf=conf)

rdd = sc.textFile("PATH/to.csv")

headers = rdd.first()

rdd = rdd.filter(lambda x: x!=headers)
rdd = rdd.map(lambda x: x.split(',')
```

```python
#3
columns = headers.split(',')
############not so good
dfRdd = rdd.toDF(columns)sa
dfRdd.show()
dfRdd.printSchema()
```

```python
from pyspark.sql.types import StructType, StructField, StringType, IntegerType #and any other types

schema = StructType([
											StructField("age", IntegerType() , True),
											StructField("gender", StringType() , True),
											StructField("name", StringType() , True),
											StructField("course", StringType() , True),
											StructField("roll", StringType() , True),
											StructField("marks", IntegerType() , True),
											StructField("email", StringType() , True),
											
])
#4 better / situational
dfRdd = spark.createDataFrame(rdd, schema=schema)
dfRdd.show()
dfRdd.printSchema()
```

## Some problems

For RDDs, if you did not change column to the schema data type, it may cause and exception. Change it before, assign the dfRdd variable with the schema (in the RDD).

# Select DF columns

```python
#1
df.select("gender","name").show()

#2
df.select(df.name,df.email).show()

#3
form pyspark.sql.functions import col

df.select(col("roll"), col("name")).show()

#4 All
df.select('*').show()

#5
df.select(df.columns[0:2]).show()

#6
# combinations

#0 Assignation
```

# withColumn in DataFrame

```python
from pyspark.sql.functions import col, lit

#1
df = df.withColumn("roll", col("roll").cast("String"))

df.printSchema()

#2 
df = df.withColumn("marks", col("marks") + 10)
df.show()

#3
df = df.withColumn("added marks", col("marks") - 10)
df.show()

#4
df = df.withColumn("Country", lit("USA"))
df.show()

#5
df = df.withColumn("marks", col("marks") + 10).withColumn("updated marks", col("marks") - 10) #.etc
df.show()
```

# withColumnRenamed in DataFrame

```python
#1
df = df.withColumnRenamed("gender", "sex")
df.show()

#2
df.select(col("name").alias("Full Name")).show()
```

# filter/where in DataFrame

```python
#1
df.filter(df.course == "DB").show()

#2
df.filter(col("course") == "DB").show()

#3
df.filter((df.course == "DB") & (df.marks > 50)).show()

#4 Not recommended
df.filter((df.course == "DB") | (df.course > "Cloud")).show()

#5 Better
courses = ["DB", "Cloud", "OOP", "DSA"]
df.filter(df.course.isin(courses)).show()

#Other
df.filter(df.course.startswith("D")).show()
df.filter(df.name.endswith("e")).show()
df.filter(df.name.contains("se")).show()
df.filter(df.name.like("%se%")).show
```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%201.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%201.png)

1.

```python
from pyspark.sql.functions import col, lit

df = df.withColumn("total marks", lit(120))
```

2.

```python
df = df.withColumn("average", col("marks") / col("total marks") * 100)
```

3.

```python
df_OOP = df.filter((df.course == "OOP") & (df.average > 80)).show()
```

4.

```python
df_Cloud = df.filter((df.course == "Cloud") & (df.average > 60)).show()
```

5.

```python
df_OOP.select("name", "marks").show()

df_Cloud.select("name", "marks").show()
```

# count, distinct, dropDuplicates in DataFrame

```python
#1 count rows
df.count()

#2
df.distinct()

#3
df.dropDuplicates(["gender"]).show()
```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%202.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%202.png)

```python
#1
df2 = df.select("age", "gender", "course")

df2.count()

df2.distinct().show()

#2
df3 = df.dropDuplicates(["age", "gender", "course"])
df3.show()
```

# sort / orderBy in DataFrame

```python
df.sort("marks").show()

df.sort("marks", "age").show()

#same
df.orderBy("marks", "age").show()

df.sort(df.marks.asc(), df.age.desc()).show()

```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%203.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%203.png)

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%204.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%204.png)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit

spark = SparkSession.builder.appName("some name").getOrCreate()

df = spark.read.options(header="True", inferSchema="True").csv("PATH/to.csv")
df.show()

#1
df2 = df.sort(df.bonus.asc())
df2.show()

#2
df3 = df.sort(df.age.desc(), df.salary.asc())
df3.show()

#3
df4 = df.sort(df.age.desc(), df.bonus.desc(), df.salary.asc())
df4.show()
```

# groupBy in DataFrame

```python
#Structure
# df.groupBy(Column).Agregation(Something)

#1
df.groupBy("gender").sum("marks").show()

#2
df.groupBy("gender").count().show()
df.groupBy("course").count().show()

#3
df.groupBy("gender").max("marks").show()
df.groupBy("gender").min("marks").show()

#4
df.groupBy("age").avg("marks").show()

#5
df.groupBy("gender").mean("marks").show()
```

### Multiple columns

```python
df.groupBy("course", "gender").count().show()
```

```python
from pyspark.sql.functions import sum,avg,max,min,mean,count

df.groupBy("course","gender").agg(count("*"), sum("marks")).show()

df.groupBy("course","gender").agg(count("*").alias("total_enrollments"), sum("marks").alias("total_marks")).show()
```

### Filtering

```python
#1
df.filter(df.gender == "Male").groupBy("gender").agg(count("*")).show()
```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%205.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%205.png)

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
from pyspark.sql.functions import sum,avg,max,min,mean,count

spark = SparkSession.builder.appName("Spark Dataframes"). getOrCreate()

df = spark.read.options(header="True", inferSchema="True").csv("PATH/to/StudentsData.csv")
df.show()
```

1

```python
df.groupBy("course").count().show()
df.groupBy("course").agg(count("*").alias("total_enrollments")).show()
```

2

```python
df.groupBy("course", "gender").agg(count("*").alias("total_enrollments")).show()
```

3

```python
df.groupBy("course", "gender").agg(sum("marks")).show()
```

4

```python
df.groupBy("course", "age").agg(min("marks"), max("marks"), avg("marks"))
```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%206.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%206.png)

```python
df = spark.read.options().csv("PATH/to/WordData.csv")
df.show()

#the column name is value

df2 = df.groupBy("value").agg(count("*").alias("word_count"))
```

# UDFs in DataFrame

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, udf
from pyspark.sql.types import IntegerType

spark = SparkSession.builder.appName("Spark Dataframes").getOrCreate()

df = spark.read.options(header="True", inferSchema="True").csv("PATH/to/OfficeData.csv")
df.show()

###
###
###

def get_total_salary(salary, bonus):
	return salary + bonus

totalSalaryUDF = udf(lambda x,y: get_total_salary(x,y), IntegerType)

df.withColumn("total_salary", totalSalaryUDF(df.salary, df.bonus)).show()
```

# Quiz

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%207.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%207.png)

```python
def get_increment(state, salary, bonus);
	if state == "NY":
		sum = salary*.10
		sum += bonus*.05
	elif state == "CA":
		sum = salary*.12
		sum += bonus*.03
	return sum

incrementUDF = udf(lambda x,y,z: get_increment(x,y,z), DoubleType)

df.withColumn("increment", incrementUDF(df.state, df.salary, df.bonus)).show()
```

# Cache and Persist

![S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%208.png](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%208.png)

```python
df.cache()
```

# DF to RDD

```python
type(df)

rdd = df.rdd

type(rdd)

rdd.collect()

rdd.filter(lambda x: x[1] == "Male").collect()

rdd.filter(lambda x: x["age"] == 20).collect()
```

# Spark SQL

```python
df.createOrReplaceTempView("Student")

df = spark.sql("select * from Student")

df.show()
```

# Writing modes

```python
#overwrite
#append
#ignore
#error
df.write.mode("overwrite").options().csv("destiny/PATH.csv")
```

# Project

![Untitled](S4%20Spark%20DataFrames%20789d0595b184402ab9a4f4709036c33b/Untitled%209.png)

```python
#1 
df.count()

#2
df.groupBy("department").count().count()
df.select("department").dropDuplicates("department").count()

#3
df.select("department").dropDuplicates("department").show()

#4
df.groupBy("department").count().show()

#5
df.groupBy("state").count().show()

#6
df.groupBy("state", "department").count().show()

#7
df = df.groupBy("department").agg(min("salary"))
df = df.sort(df.salary.asc()).show()

df = df.groupBy("department").agg(max("salary"))
df = df.sort(df.salary.asc()).show()

df.groupBy("department").agg(min("salary").alias("min"), max("salary").alias("max")).orderBy(col("max").asc(), col("min").asc()).show()

#8
avg_bonus_NY = df.filter(df.state == "NY").groupBy("state").agg(avg("bonus").alias("avg_bonus")).select("avg_bonus").collect()[0]["avg_bonus"] #important: extracts df's data 
df.filter((df.state == "NY") & (df.department == "Finance") &(df.bonus > avg_bonus_NY)).select(df.employee).show()

#9
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit, udf
from pyspark.sql.types import IntegerType

###
###
###

def incr_salary(salary, age):
	if age > 45:
		return salary + 500
	return salary

incr_salaryyUDF = udf(lambda x,y: incr_salary(x,y), IntegerType)

df.withColumn("salary", increase_elders_salaryUDF(df.salary, df.age)).show()

#10
df2 = df.filter(df.age > 45)
df.write.mode("overwrite").options().csv("PATH")

```

#